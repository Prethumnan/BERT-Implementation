{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6VOd0unuI4a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qpJHcJ4_SO9",
        "outputId": "bb2a3235-96e8-4486-f635-2dbb144a651b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kG4RQv4Iuj1S",
        "outputId": "973333d5-9271-4356-c0af-49d0b6380057"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      611857364396965889  \\\n",
              "0     614484565059596288   \n",
              "1     614746522043973632   \n",
              "2     614877582664835073   \n",
              "3     611932373039644672   \n",
              "4     611570404268883969   \n",
              "...                  ...   \n",
              "3079  613678555935973376   \n",
              "3080  613294681225621504   \n",
              "3081  615246897670922240   \n",
              "3082  613016084371914753   \n",
              "3083  611566876762640384   \n",
              "\n",
              "     @aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap  \\\n",
              "0     Dorian Gray with Rainbow Scarf #LoveWins (from...                              \n",
              "1     @SelectShowcase @Tate_StIves ... Replace with ...                              \n",
              "2     @Sofabsports thank you for following me back. ...                              \n",
              "3     @britishmuseum @TudorHistory What a beautiful ...                              \n",
              "4     @NationalGallery @ThePoldarkian I have always ...                              \n",
              "...                                                 ...                              \n",
              "3079  MT @AliHaggett: Looking forward to our public ...                              \n",
              "3080                    @britishmuseum Upper arm guard?                              \n",
              "3081           @MrStuchbery @britishmuseum Mesmerising.                              \n",
              "3082  @NationalGallery The 2nd GENOCIDE against #Bia...                              \n",
              "3083  @britishmuseum Experience #battlewaterloo from...                              \n",
              "\n",
              "            nocode  \n",
              "0            happy  \n",
              "1            happy  \n",
              "2            happy  \n",
              "3            happy  \n",
              "4            happy  \n",
              "...            ...  \n",
              "3079         happy  \n",
              "3080        nocode  \n",
              "3081         happy  \n",
              "3082  not-relevant  \n",
              "3083        nocode  \n",
              "\n",
              "[3084 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-884753fe-0cf4-46fd-abb5-a5677f29a2a1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>611857364396965889</th>\n",
              "      <th>@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap</th>\n",
              "      <th>nocode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>614484565059596288</td>\n",
              "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>614746522043973632</td>\n",
              "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>614877582664835073</td>\n",
              "      <td>@Sofabsports thank you for following me back. ...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>611932373039644672</td>\n",
              "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>611570404268883969</td>\n",
              "      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3079</th>\n",
              "      <td>613678555935973376</td>\n",
              "      <td>MT @AliHaggett: Looking forward to our public ...</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3080</th>\n",
              "      <td>613294681225621504</td>\n",
              "      <td>@britishmuseum Upper arm guard?</td>\n",
              "      <td>nocode</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3081</th>\n",
              "      <td>615246897670922240</td>\n",
              "      <td>@MrStuchbery @britishmuseum Mesmerising.</td>\n",
              "      <td>happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3082</th>\n",
              "      <td>613016084371914753</td>\n",
              "      <td>@NationalGallery The 2nd GENOCIDE against #Bia...</td>\n",
              "      <td>not-relevant</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3083</th>\n",
              "      <td>611566876762640384</td>\n",
              "      <td>@britishmuseum Experience #battlewaterloo from...</td>\n",
              "      <td>nocode</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3084 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-884753fe-0cf4-46fd-abb5-a5677f29a2a1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-884753fe-0cf4-46fd-abb5-a5677f29a2a1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-884753fe-0cf4-46fd-abb5-a5677f29a2a1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data=pd.read_csv(\"smile-annotations-final.csv\")\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n4NPFOT5lMD"
      },
      "outputs": [],
      "source": [
        "data.rename(columns={\"nocode\":\"emotion\"},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNBezXsButOG",
        "outputId": "ad048a7e-48cd-43f4-a9db-297dbc42c967"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nocode               1571\n",
              "happy                1137\n",
              "not-relevant          214\n",
              "angry                  57\n",
              "surprise               35\n",
              "sad                    32\n",
              "happy|surprise         11\n",
              "happy|sad               9\n",
              "disgust|angry           7\n",
              "disgust                 6\n",
              "sad|disgust             2\n",
              "sad|angry               2\n",
              "sad|disgust|angry       1\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data[\"emotion\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3LOKpcL3RGP"
      },
      "outputs": [],
      "source": [
        "#Considering columns which has only valid emotions\n",
        "df = data[~data.emotion.str.contains('\\|')]\n",
        "df = df[df.emotion != 'nocode']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0rVUloB3kd7",
        "outputId": "dfab3c3b-a4a3-422e-eb14-0881b220496b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "happy           1137\n",
              "not-relevant     214\n",
              "angry             57\n",
              "surprise          35\n",
              "sad               32\n",
              "disgust            6\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.emotion.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amRZeV1T4pJw",
        "outputId": "13a5eda5-6c78-4219-f0ce-7b8cae596933"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['happy', 'not-relevant', 'angry', 'disgust', 'sad', 'surprise'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "labels=df.emotion.unique()\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H4WZn_Y4xUi",
        "outputId": "c5149edd-d3a6-4eca-e51b-72432aaf3949"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'happy': 0,\n",
              " 'not-relevant': 1,\n",
              " 'angry': 2,\n",
              " 'disgust': 3,\n",
              " 'sad': 4,\n",
              " 'surprise': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "label_dict = {}\n",
        "for index, possible_label in enumerate(labels):\n",
        "    label_dict[possible_label] = index\n",
        "\n",
        "label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "_nlaWecY6jJt",
        "outputId": "77bb5b5a-1b37-4e30-b836-432e48607386"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      611857364396965889  \\\n",
              "0     614484565059596288   \n",
              "1     614746522043973632   \n",
              "2     614877582664835073   \n",
              "3     611932373039644672   \n",
              "4     611570404268883969   \n",
              "...                  ...   \n",
              "3077  611258135270060033   \n",
              "3078  612214539468279808   \n",
              "3079  613678555935973376   \n",
              "3081  615246897670922240   \n",
              "3082  613016084371914753   \n",
              "\n",
              "     @aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap  \\\n",
              "0     Dorian Gray with Rainbow Scarf #LoveWins (from...                              \n",
              "1     @SelectShowcase @Tate_StIves ... Replace with ...                              \n",
              "2     @Sofabsports thank you for following me back. ...                              \n",
              "3     @britishmuseum @TudorHistory What a beautiful ...                              \n",
              "4     @NationalGallery @ThePoldarkian I have always ...                              \n",
              "...                                                 ...                              \n",
              "3077  @_TheWhitechapel @Campaignforwool @SlowTextile...                              \n",
              "3078  “@britishmuseum: Thanks for ranking us #1 in @...                              \n",
              "3079  MT @AliHaggett: Looking forward to our public ...                              \n",
              "3081           @MrStuchbery @britishmuseum Mesmerising.                              \n",
              "3082  @NationalGallery The 2nd GENOCIDE against #Bia...                              \n",
              "\n",
              "           emotion  label  \n",
              "0            happy      0  \n",
              "1            happy      0  \n",
              "2            happy      0  \n",
              "3            happy      0  \n",
              "4            happy      0  \n",
              "...            ...    ...  \n",
              "3077  not-relevant      1  \n",
              "3078         happy      0  \n",
              "3079         happy      0  \n",
              "3081         happy      0  \n",
              "3082  not-relevant      1  \n",
              "\n",
              "[1481 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1e82440-f9a4-462a-b0b5-83e6646c2835\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>611857364396965889</th>\n",
              "      <th>@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap</th>\n",
              "      <th>emotion</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>614484565059596288</td>\n",
              "      <td>Dorian Gray with Rainbow Scarf #LoveWins (from...</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>614746522043973632</td>\n",
              "      <td>@SelectShowcase @Tate_StIves ... Replace with ...</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>614877582664835073</td>\n",
              "      <td>@Sofabsports thank you for following me back. ...</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>611932373039644672</td>\n",
              "      <td>@britishmuseum @TudorHistory What a beautiful ...</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>611570404268883969</td>\n",
              "      <td>@NationalGallery @ThePoldarkian I have always ...</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3077</th>\n",
              "      <td>611258135270060033</td>\n",
              "      <td>@_TheWhitechapel @Campaignforwool @SlowTextile...</td>\n",
              "      <td>not-relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3078</th>\n",
              "      <td>612214539468279808</td>\n",
              "      <td>“@britishmuseum: Thanks for ranking us #1 in @...</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3079</th>\n",
              "      <td>613678555935973376</td>\n",
              "      <td>MT @AliHaggett: Looking forward to our public ...</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3081</th>\n",
              "      <td>615246897670922240</td>\n",
              "      <td>@MrStuchbery @britishmuseum Mesmerising.</td>\n",
              "      <td>happy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3082</th>\n",
              "      <td>613016084371914753</td>\n",
              "      <td>@NationalGallery The 2nd GENOCIDE against #Bia...</td>\n",
              "      <td>not-relevant</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1481 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1e82440-f9a4-462a-b0b5-83e6646c2835')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1e82440-f9a4-462a-b0b5-83e6646c2835 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1e82440-f9a4-462a-b0b5-83e6646c2835');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df['label'] = df[\"emotion\"].replace(label_dict)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "QkFjQICe6sGs",
        "outputId": "23d8a0f8-e7c8-4877-c85d-4bfc1387cd56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c7b2035b0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "sns.countplot(df[\"emotion\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrDs5Zji7ZKK"
      },
      "outputs": [],
      "source": [
        "df.rename(columns={\"@aandraous @britishmuseum @AndrewsAntonio Merci pour le partage! @openwinemap\":\"tweet\"},inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "BtMRiBAr7rHx",
        "outputId": "567c4b9f-633c-4af5-a0b5-a19f50679c04"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#plot hist of sentence length\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot([len(s) for s in df[\"tweet\"]], bins=100)\n",
        "plt.title('Sentence Length')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmDDjd7u7zsq",
        "outputId": "9e9784de-8528-49a2-924f-67628d2f4c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max length:  149\n"
          ]
        }
      ],
      "source": [
        "#find the maximum length\n",
        "max_len = max([len(sent) for sent in df[\"tweet\"]])\n",
        "print('Max length: ', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8jc_8sj76B-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df[\"tweet\"], \n",
        "                                                   df[\"label\"],\n",
        "                                                   test_size = 0.15,\n",
        "                                                   random_state = 17,\n",
        "                                                   stratify = df[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2mv7tEw-vFa"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "                                         do_lower_case = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm4LPmpC_cST"
      },
      "outputs": [],
      "source": [
        "train=pd.DataFrame({\"Text\":X_train,\"emotion\":y_train})\n",
        "test= pd.DataFrame({\"Text\":X_test,\"emotion\":y_test})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQKEB8NxMIvA"
      },
      "source": [
        "Just like the other models, we have to convert the words into vectors. For that purpose we are using a bert method to do that task.\n",
        "\n",
        "STEPS:\n",
        "\n",
        "1.Adding special tokens (CLS and SEP). CLS is added in begining and SEP is added at the end.\n",
        "\n",
        "eg: [CLS] Sentence A [SEP] Sentence B [SEP]\n",
        "\n",
        "2.Then we are converting the sentences into a sequence of number\n",
        "\n",
        "3.Also doing the masking operation, the output will be pytorch tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4faF5RunAtE1"
      },
      "outputs": [],
      "source": [
        "#encode train set\n",
        "#Converting each tweet into a sequence of numbers with the length of 256\n",
        "#Output will be a dictionary of input_ids, token_type_ids and attention_mask\n",
        "#Attention mask contains 0 as masked token and 1 as real token\n",
        "encoded_data_train = tokenizer.batch_encode_plus(train[\"Text\"].values,          \n",
        "                                                add_special_tokes = True,\n",
        "                                                return_attention_mask = True,\n",
        "                                                pad_to_max_length = True,\n",
        "                                                max_length = 256,\n",
        "                                                return_tensors = 'pt')\n",
        "                                                \n",
        "#encode validation set\n",
        "encoded_data_val = tokenizer.batch_encode_plus(test[\"Text\"].values,\n",
        "                                                add_special_tokes = True,\n",
        "                                                return_attention_mask = True,\n",
        "                                                pad_to_max_length = True,\n",
        "                                                max_length = 256,\n",
        "                                                return_tensors = 'pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqe_slfmA9e_",
        "outputId": "bc562bd0-03b0-4458-a809-315de6527ce6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101,  1030,  4459,  ...,     0,     0,     0],\n",
              "        [  101,  1030, 12323,  ...,     0,     0,     0],\n",
              "        [  101,  1037,  2261,  ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101,  1030,  2329,  ...,     0,     0,     0],\n",
              "        [  101,  1030, 12869,  ...,     0,     0,     0],\n",
              "        [  101,  1012,  1030,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "encoded_data_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIIuKufMKvRo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "#train set\n",
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(train[\"emotion\"].values)\n",
        "\n",
        "#validation set\n",
        "input_ids_val = encoded_data_val['input_ids']\n",
        "attention_masks_val = encoded_data_val['attention_mask']\n",
        "labels_val = torch.tensor(test[\"emotion\"].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd5Ot9xVLlfL",
        "outputId": "36f4aa8d-cb6e-4903-b35e-7386a6b8842d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0,  ..., 0, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "labels_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_DfVm79NqdF"
      },
      "source": [
        "Now we will set up our BERT model through 4 steps:\n",
        "\n",
        "1.Load Pre-trained BERT\n",
        "\n",
        "2.Create DataLoader\n",
        "\n",
        "3.Set up optimizer\n",
        "\n",
        "4.Set up scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD3vPnESN6ih",
        "outputId": "6a9df829-674a-4e98-b4ae-93a9938628f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#1.Loading pre-trained model\n",
        "from transformers import BertForSequenceClassification\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                      num_labels = len(label_dict),\n",
        "                                                      output_attentions = False,\n",
        "                                                      output_hidden_states = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auyOMg3hO9fb"
      },
      "outputs": [],
      "source": [
        "#2.Create dataloader\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "#train set\n",
        "dataset_train = TensorDataset(input_ids_train, \n",
        "                              attention_masks_train,\n",
        "                              labels_train)\n",
        "\n",
        "#validation set\n",
        "dataset_val = TensorDataset(input_ids_val, \n",
        "                             attention_masks_val, \n",
        "                             labels_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4s6RQihSwKO"
      },
      "outputs": [],
      "source": [
        "#Creating iterator for ourdataset to save memory during training and boost the training speed.\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "#train set\n",
        "dataloader_train = DataLoader(dataset_train,\n",
        "                              sampler = RandomSampler(dataset_train),\n",
        "                              batch_size = batch_size)\n",
        "\n",
        "#validation set\n",
        "dataloader_val = DataLoader(dataset_val,\n",
        "                              sampler = RandomSampler(dataset_val),\n",
        "                              batch_size = 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNbmnvNFUQWW"
      },
      "source": [
        "* Optimizers are used in BERT (and in deep learning in general) to update the model parameters in the direction that minimizes the loss function. \n",
        "\n",
        "* Schedulers are used to adjust the learning rate during training, typically reducing the learning rate over time so that the optimizer can converge more efficiently to the optimal solution. \n",
        "\n",
        "* By using an optimizer and a scheduler, the model can converge faster and achieve better results than if only one or the other is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2as4h8I4TOw_",
        "outputId": "ec2f72a0-cf90-4525-cff6-2bb5671df116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#setup optimizer and scheduler\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                 lr = 1e-5,\n",
        "                 eps = 1e-8) \n",
        "                 \n",
        "epochs = 10\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                           num_warmup_steps = 0,\n",
        "                                           num_training_steps = len(dataloader_train)*epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA6ri1jBUfmU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "def evaluate(dataloader_val):\n",
        "\n",
        "    #evaluation mode \n",
        "    model.eval()\n",
        "    \n",
        "    #tracking variables\n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    for batch in tqdm(dataloader_val):\n",
        "        \n",
        "        #load into GPU\n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        #define inputs\n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2]}\n",
        "\n",
        "        #compute logits\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "        \n",
        "        #compute loss\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        #compute accuracy\n",
        "        logits = logits.detach().cpu().numpy() #converting tensors to numpy array\n",
        "        label_ids = inputs['labels'].cpu().numpy() #converting tensors to numpy array\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    \n",
        "    #compute average loss\n",
        "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "            \n",
        "    return loss_val_avg, predictions, true_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuQaQflOVuMc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def f1_score_func(preds, labels):\n",
        "    preds_flat = np.argmax(preds, axis = 1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, preds, average = 'weighted')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ6LqHQiVxYg"
      },
      "outputs": [],
      "source": [
        "#accuracy score\n",
        "def accuracy_per_class(preds, labels):\n",
        "\n",
        "  model.eval()\n",
        "  correct_predictions=0\n",
        "  total_predictions = 0\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    for input_ids, attention_masks, labels in dataloader_val:\n",
        "      inputs = {\n",
        "                'input_ids': input_ids.to(device),\n",
        "                'attention_mask': attention_masks.to(device)\n",
        "            }\n",
        "\n",
        "      outputs = model(**inputs)\n",
        "      logits = outputs[1]\n",
        "\n",
        "      _, predicted_labels = torch.max(logits, dim=1) #Return max_value and index of the max_value\n",
        "      correct_predictions += int((predicted_labels == labels.to(device)).sum()) #example output will be tensor(2) ----> int(2)\n",
        "      total_predictions += labels.size(0) #Here labels.size(0) returns 32(batch size)\n",
        "    \n",
        "  accuracy = correct_predictions / total_predictions\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "seed_val = 17\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "metadata": {
        "id": "En9g3ieuw3_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKC0EBl9V09y"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "for epoch in range(1, epochs+1):\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    accumulation_steps = 10\n",
        "    \n",
        "    loss_train_total = 0\n",
        "    \n",
        "    steps=0\n",
        "    \n",
        "    for input_ids,attention_masks,labels in dataloader_train:\n",
        "        \n",
        "        model.zero_grad() #set gradient to 0\n",
        "        #since we are processing batch by batch, we set gradiant to zero initially.If we does not set to zero, then while computing gradient for 2nd batch\n",
        "        #New gradient=1st batch gradient + 2nd batch gradient, to avoid this, we set gradient to zero\n",
        "    \n",
        "        \n",
        "        #define inputs and loading it into CPU\n",
        "        inputs = {'input_ids':input_ids.to(device),\n",
        "                  'attention_mask':attention_masks.to(device),    #input=In simply,this a single tweet posted by a person\n",
        "                  'labels':labels.to(device)}\n",
        "        \n",
        "        outputs = model(**inputs) #unpack the dict straight into inputs\n",
        "        #output=(loss,logits)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        if (steps + 1) % accumulation_steps == 0:\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            model.zero_grad()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        steps += 1\n",
        "        #loss.backward() #computes the gradients of the loss with respect to the model parameters / slope \n",
        "        \n",
        "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # This make sure that gradient value does not exceed more than 1\n",
        "        \n",
        "        #optimizer.step() # calculate new_weight=old_weight- learning rate * derivative of loss\n",
        "        #scheduler.step() # It is used to update the learning rate of the optimizer\n",
        "        \n",
        "    torch.save(model.state_dict(), f'Models/ BERT_ft_epoch{epoch}.model')\n",
        "    \n",
        "    taining_loss = loss_train_total / len(dataloader_train)\n",
        "    loss_val_avg, predictions, true_vals = evaluate(dataloader_val)\n",
        "    val_f1 = f1_score_func(predictions, true_vals)\n",
        "\n",
        "    print('Epoch :',epoch)\n",
        "    print('Training loss :', taining_loss)\n",
        "    print('Validation loss :', loss_val_avg)\n",
        "    print('F1 Score (weighted) :',val_f1)\n",
        "    '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_train_total = 0\n",
        "    \n",
        "    progress_bar = tqdm(dataloader_train, \n",
        "                        desc = 'Epoch {:1d}'.format(epoch), \n",
        "                        leave = False, \n",
        "                        disable = False)\n",
        "    \n",
        "    for input_ids,attention_masks,labels in progress_bar:\n",
        "        \n",
        "        model.zero_grad() #set gradient to 0\n",
        "        #since we are processing batch by batch, we set gradient to zero else gradient2=gradient1+gradient2(This should not happen)\n",
        "    \n",
        "        inputs = {'input_ids':input_ids.to(device),\n",
        "                  'attention_mask':attention_masks.to(device),    #input=In simply,this a single tweet posted by a person or single row\n",
        "                  'labels':labels.to(device)}\n",
        "        \n",
        "        outputs = model(**inputs) #unpack the dict straight into inputs\n",
        "        #outputs=(loss,logits)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item() #Example : loss_train_total=tensor(3) if we use loss.item then loss_train_total=3\n",
        "        loss.backward() #calculates slope / gradient with respect to loss\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # It prevent the value of gradient not exeeding 1\n",
        "        \n",
        "        optimizer.step() #updating parameters/ weights\n",
        "        scheduler.step() #Updating learning rate\n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})\n",
        "        \n",
        "    torch.save(model.state_dict(), f'Models/ BERT_ft_epoch{epoch}.model')\n",
        "    \n",
        "    tqdm.write('\\n Epoch {epoch}')\n",
        "    \n",
        "    loss_train_ave = loss_train_total / len(dataloader)\n",
        "    tqdm.write('Training loss: {loss_train_avg}')\n",
        "    \n",
        "    val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
        "    val_f1 = f1_score_func(predictions, true_vals)\n",
        "    tqdm.write(f'Validation loss: {val_loss}')\n",
        "    tqdm.write(f'F1 Score (weighted): {val_f1}')"
      ],
      "metadata": {
        "id": "_d3If8DLxgyd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}